{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CARL TF 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJbYXou6chZf",
        "outputId": "d969a0c4-5eea-46ef-ff10-9ea01fb02eb4"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr  1 08:25:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY8Fb1pg4R_7",
        "outputId": "b6702b7b-b65b-49b0-ac47-7f49a53e86c0"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYAQShBA4S81",
        "outputId": "f8e64060-9be0-4de3-9b6b-386b03a94735"
      },
      "source": [
        "# %cd /content/drive/MyDrive/S2 MIK/Tesis Raditya Nurfadillah/CARL\n",
        "# %cd /content/drive/MyDrive/Tesis Raditya Nurfadillah/CARL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/S2 MIK/Tesis Raditya Nurfadillah/CARL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBqdJjP94ZWy",
        "outputId": "28845223-a848-4623-e771-74dc9642b95a"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " aiv\n",
            " aiv.tar\n",
            "'CARL BERT.ipynb'\n",
            "'CARL FastText 2.ipynb'\n",
            "'CARL FastText Document Average.ipynb'\n",
            "'CARL FastText.ipynb'\n",
            "'CARL FastText (Surprise + Review-based) 2.ipynb'\n",
            "'CARL FastText (Surprise + Review-based) High Computing Power 2.ipynb'\n",
            "'CARL FastText (Surprise + Review-based) High Computing Power 3.ipynb'\n",
            "'CARL FastText (Surprise + Review-based) High Computing Power.ipynb'\n",
            "'CARL FastText (Surprise + Review-based).ipynb'\n",
            "'CARL FemaleDaily.ipynb'\n",
            "'CARL High Computing Power.ipynb'\n",
            " CARL.ipynb\n",
            "'CARL Tensorflow 2 - 2.ipynb'\n",
            "'CARL Tensorflow 2 - 3.ipynb'\n",
            "'CARL Tensorflow 2.ipynb'\n",
            " cc.en.300.bin\n",
            " cc.id.300.bin\n",
            "'Copy of CARL.ipynb'\n",
            "'Copy of Copy of CARL High Computing Power.ipynb'\n",
            "'Copy of Salinan dari P100.ipynb'\n",
            " Dataset\n",
            "'Download CARL Dataset.ipynb'\n",
            "'EDA Amazon + FemaleDaily.ipynb'\n",
            " EDA.ipynb\n",
            " Model\n",
            "'prediction beauty.csv'\n",
            "'Prediction CARL'\n",
            "'prediction FemaleDaily.csv'\n",
            "'prediction FemaleDaily epoch 0.csv'\n",
            "'prediction FemaleDaily epoch 100.csv'\n",
            "'prediction FemaleDaily epoch 150.csv'\n",
            "'prediction FemaleDaily epoch 50.csv'\n",
            "'prediction music.csv'\n",
            "'prediction office.csv'\n",
            "'prediction patio.csv'\n",
            " prediction_patio.csv\n",
            "'prediction patio epoch 0.csv'\n",
            "'prediction sports.csv'\n",
            "'prediction sports epoch 0.csv'\n",
            "'prediction sports epoch 100.csv'\n",
            "'prediction sports epoch 150.csv'\n",
            "'prediction sports epoch 50.csv'\n",
            "'Preprocess Dataset.ipynb'\n",
            " SBERT.ipynb\n",
            " sbert_sentence-transformers-paraphrase-xlm-r-multilingual-v1-2021-03-12_07-26-45\n",
            " sbert_sentence-transformers-paraphrase-xlm-r-multilingual-v1-2021-03-12_07-43-42\n",
            " sbert_sentence-transformers-paraphrase-xlm-r-multilingual-v1-2021-03-12_07-45-10\n",
            " sbert_sentence-transformers-paraphrase-xlm-r-multilingual-v1-2021-03-12_07-55-05\n",
            " shopee.csv\n",
            " shopee_food_29012021.csv\n",
            "'Surprise BLB + Baseline Average.ipynb'\n",
            "'Test FastText.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkicuhkdBy96"
      },
      "source": [
        "# %tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn9IE9QE4Z-s",
        "outputId": "0660b194-58ef-4646-c283-b895e2a57477"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMx9_xJdB8CG"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Set a seed value\n",
        "seed_value = 24\n",
        "tf.random.set_seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lGI7kqcm8U3",
        "outputId": "eda7c8c5-dff2-417e-9ee5-8fe6fbb45305"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkP52mXC4ega"
      },
      "source": [
        "# Extract Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITKjNaGQ4cV7"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "class Dataset(object):\n",
        "    \"'extract dataset from file'\"\n",
        "\n",
        "    def __init__(self, max_length, path, word_id_path):\n",
        "        self.word_id_dict = self.load_word_dict(path + word_id_path)\n",
        "        print(\"wordId_dict finished\")\n",
        "        self.userReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"UserReviews.out\")\n",
        "        self.itemReview_dict = self.load_reviews(max_length, len(self.word_id_dict), path + \"ItemReviews.out\")\n",
        "        print(\"load reviews finished\")\n",
        "        self.num_users, self.num_items = len(self.userReview_dict), len(self.itemReview_dict)\n",
        "        print(self.num_users, self.num_items)\n",
        "        self.trainMtrx = self.load_ratingFile_as_mtrx(path + \"TrainInteraction.out\")\n",
        "        self.valRatings = self.load_ratingFile_as_list(path + \"ValInteraction.out\")\n",
        "        self.testRatings = self.load_ratingFile_as_list(path + \"TestInteraction.out\")\n",
        "\n",
        "    def load_word_dict(self, path):\n",
        "        wordId_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                wordId_dict[arr[0]] = int(arr[1])\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "\n",
        "        return wordId_dict\n",
        "\n",
        "    def load_reviews(self, max_doc_length, padding_word_id, path):\n",
        "        entity_review_dict = {}\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            line = f.readline().replace(\"\\n\", \"\")\n",
        "            while line != None and line != \"\":\n",
        "                review = []\n",
        "                arr = line.split(\"\\t\")\n",
        "                entity = int(arr[0])\n",
        "                word_list = arr[1].split(\" \")\n",
        "\n",
        "                for i in range(len(word_list)):\n",
        "                    # if (word_list[i] == \"\" or word_list[i] == None or (not self.word_id_dict.has_key(word_list[i]))):\n",
        "                    if (word_list[i] == \"\" or word_list[i] == None or (not word_list[i] in self.word_id_dict)):\n",
        "                        continue\n",
        "                    review.append(self.word_id_dict.get(word_list[i]))\n",
        "                    if (len(review) >= max_doc_length):\n",
        "                        break\n",
        "                if (len(review) < max_doc_length):\n",
        "                    review = self.padding_word(max_doc_length, padding_word_id, review)\n",
        "                entity_review_dict[entity] = review\n",
        "                line = f.readline().replace(\"\\n\", \"\")\n",
        "        return entity_review_dict\n",
        "\n",
        "    def padding_word(self, max_size, max_word_idx, review):\n",
        "        review.extend([max_word_idx]*(max_size - len(review)))\n",
        "        return review\n",
        "\n",
        "    def load_ratingFile_as_mtrx(self, file_path):\n",
        "        mtrx = sp.dok_matrix((self.num_users, self.num_items), dtype=np.float32)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            line = line.strip()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mtrx[user, item] = rating\n",
        "                line = f.readline()\n",
        "\n",
        "        return mtrx\n",
        "\n",
        "    def load_ratingFile_as_list(self, file_path):\n",
        "        rateList = []\n",
        "\n",
        "        with open(file_path, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                rate = float(arr[2])\n",
        "                rateList.append([user, item, rate])\n",
        "                line = f.readline()\n",
        "\n",
        "        return rateList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5177vCZ4jgG"
      },
      "source": [
        "# Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usPG85XS4i_P"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_test_list(batch_size, test_rating, user_reviews, item_reviews):\n",
        "    user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs = [], [], [], [], []\n",
        "    for count in range(int(math.ceil(len(test_rating) / float(batch_size)))):\n",
        "        user_test, item_test, user_input_test, item_input_test, rating_input_test = [], [], [], [], []\n",
        "        for idx in range(batch_size):\n",
        "            index = (count * batch_size + idx)\n",
        "            if (index >= len(test_rating)):\n",
        "                break\n",
        "            rating = test_rating[index]\n",
        "            user_test.append(rating[0])\n",
        "            item_test.append(rating[1])\n",
        "            user_input_test.append(user_reviews.get(rating[0]))\n",
        "            item_input_test.append(item_reviews.get(rating[1]))\n",
        "            rating_input_test.append([rating[2]])\n",
        "        user_test_batchs.append(user_test)\n",
        "        item_test_batchs.append(item_test)\n",
        "        user_input_test_batchs.append(user_input_test)\n",
        "        item_input_test_batchs.append(item_input_test)\n",
        "        rating_input_test_batchs.append(rating_input_test)\n",
        "        #print count, len(item_input_test_batchs[count])\n",
        "    return user_test_batchs, item_test_batchs, user_input_test_batchs, item_input_test_batchs, rating_input_test_batchs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxkh3s2OzSBy"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2duRt-BQnXJQ"
      },
      "source": [
        "# Download dan unzip dataset\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OesMFB3FnawD"
      },
      "source": [
        "!gunzip cc.en.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ViXpBszT23",
        "outputId": "f81ec993-4c30-4e60-c2d2-4dee3d111e5e"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.7/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (54.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.19.5)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext) (2.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zkVgKmnzV0z",
        "outputId": "81cce3a4-2f92-47cf-e262-3ffeebd77207"
      },
      "source": [
        "import fasttext\n",
        "\n",
        "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWibytby5K40"
      },
      "source": [
        "# Interaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHyqn8oDUPBu"
      },
      "source": [
        "def get_train_instance(train):\n",
        "    user_input, item_input, rates = [], [], []\n",
        "\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        rates.append(train[u,i])\n",
        "    return user_input, item_input, rates\n",
        "\n",
        "def get_train_instance_batch_change(count, batch_size, user_input, item_input, ratings, user_reviews, item_reviews):\n",
        "    users_batch, items_batch, user_input_batch, item_input_batch, labels_batch = [], [], [], [], []\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        index = (count*batch_size + idx) % len(user_input)\n",
        "        users_batch.append(user_input[index])\n",
        "        items_batch.append(item_input[index])\n",
        "        user_input_batch.append(user_reviews.get(user_input[index]))\n",
        "        item_input_batch.append(item_reviews.get(item_input[index]))\n",
        "        labels_batch.append([ratings[index]])\n",
        "\n",
        "    return users_batch, items_batch, user_input_batch, item_input_batch, labels_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh7T2-o29TlF",
        "outputId": "7fc67a69-2ec8-476b-99d1-49ffe7e20782"
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "import math, os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "word_latent_dim = 300\n",
        "latent_dim = 30\n",
        "max_doc_length = 300\n",
        "windows = 3\n",
        "v_dim = 50\n",
        "learning_rate = 0.001\n",
        "lambda_1 = 0.05\n",
        "batch_size = 100\n",
        "epochs = 180\n",
        "\n",
        "# loading data\n",
        "firTime = time()\n",
        "dataset_path = \"Dataset/\"\n",
        "category = \"patio\"\n",
        "path = dataset_path + category + \"/\"\n",
        "dataSet = Dataset(max_doc_length, path,\"WordDict.out\")\n",
        "word_dict, user_reviews, item_reviews, train, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.testRatings\n",
        "secTime = time()\n",
        "\n",
        "num_users, num_items = train.shape\n",
        "print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "print(num_users, num_items)\n",
        "\n",
        "# get train instances\n",
        "user_input, item_input, rateings = get_train_instance(train)\n",
        "\n",
        "# get test instances\n",
        "user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wordId_dict finished\n",
            "load reviews finished\n",
            "35598 18357\n",
            "load data: 32.517s\n",
            "35598 18357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "At1pYppD9Gzp"
      },
      "source": [
        "class My_Model():\n",
        "  def __init__(self):\n",
        "    self.user_entity_embedding = tf.Variable(tf.random.normal([num_users, latent_dim], mean=0, stddev=0.02), name=\"user_entity_embeddings\")\n",
        "    self.item_entity_embedding = tf.Variable(tf.random.normal([num_items, latent_dim], mean=0, stddev=0.02), name=\"item_entity_embeddings\")\n",
        "    self.w_entity_0 = tf.Variable(tf.zeros(1), name=\"entity_w_0\")\n",
        "    self.w_entity_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim*3], stddev=0.3), name=\"entity_w_1\")\n",
        "    self.v_entity = tf.Variable(tf.random.truncated_normal([latent_dim*3, v_dim], stddev=0.3), name=\"entity_v\")\n",
        "\n",
        "  def run(self, users, items):\n",
        "    user_entity_embeds = tf.nn.embedding_lookup(self.user_entity_embedding, users)\n",
        "    item_entity_embeds = tf.nn.embedding_lookup(self.item_entity_embedding, items)\n",
        "\n",
        "    entity_embeds_sum = tf.concat([tf.multiply(user_entity_embeds, item_entity_embeds), user_entity_embeds, item_entity_embeds],1)\n",
        "\n",
        "    J_e_1 = self.w_entity_0 + tf.linalg.matmul(entity_embeds_sum, self.w_entity_1, transpose_b=True)\n",
        "\n",
        "    entity_embeds_sum_1 = tf.expand_dims(entity_embeds_sum, -1)\n",
        "    entity_embeds_sum_2 = tf.expand_dims(entity_embeds_sum, 1)\n",
        "    J_e_2 = tf.reduce_sum(\n",
        "            tf.reduce_sum(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(self.v_entity, self.v_entity, transpose_b=True)),\n",
        "                          2), 1, keepdims=True)\n",
        "    J_e_3 = tf.linalg.trace(tf.multiply(tf.matmul(entity_embeds_sum_1, entity_embeds_sum_2), tf.matmul(self.v_entity, self.v_entity, transpose_b=True)))\n",
        "    J_e_total = (J_e_1 + 0.5 * (J_e_2 - J_e_3))\n",
        "    outputs = J_e_total\n",
        "    return outputs\n",
        "\n",
        "  def trainable_variables(self):\n",
        "    return [self.user_entity_embedding, self.item_entity_embedding, self.w_entity_0, self.w_entity_1, self.v_entity]\n",
        "    # self.weights + self.bias\n",
        "\n",
        "  def loss_function(self,y_pred,y_true):\n",
        "    return tf.reduce_mean(tf.math.squared_difference(y_pred, y_true)) + lambda_1 * (tf.nn.l2_loss(self.user_entity_embedding) + tf.nn.l2_loss(self.item_entity_embedding) + tf.nn.l2_loss(self.v_entity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EXZ_okRqFTvW"
      },
      "source": [
        "my_model = My_Model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e8YJsk3FBji7"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "# lr = 1e-3\n",
        "# wd = 1e-4\n",
        "# optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crO8vz6uBurG"
      },
      "source": [
        "def train_step( model, users, items , y_true, epoch):\n",
        "    \n",
        "  with tf.GradientTape() as tape:\n",
        "        \n",
        "    # Get the predictions\n",
        "    preds = model.run(users, items)\n",
        "        \n",
        "    # Calc the loss\n",
        "    current_loss = model.loss_function(preds,y_true)\n",
        "    \n",
        "    # Get the gradients\n",
        "    grads = tape.gradient(current_loss, model.trainable_variables())\n",
        "    \n",
        "    # Update the weights\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables()))\n",
        "\n",
        "    return current_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orYozyL0Dc9z"
      },
      "source": [
        "def eval_model(model, user_test, item_test, rate_tests, rmses, maes):\n",
        "\n",
        "    # predicts = sess.run(predict_rating, feed_dict={users: user_test, items: item_test})\n",
        "    predicts = model.run(user_test, item_test)\n",
        "    row, col = predicts.shape\n",
        "    for r in range(row):\n",
        "        rmses.append(pow((predicts[r, 0] - rate_tests[r][0]), 2))\n",
        "        maes.append(abs(predicts[r, 0] - rate_tests[r][0]))\n",
        "    return rmses, maes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUds1FNYEBDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281582f6-6a98-4cdd-d894-116eb0e1beb0"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  t = time()\n",
        "  loss_total = 0.0\n",
        "  count = 0.0          \n",
        "  for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "    user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input,\n",
        "                                                                                                    item_input, rateings,\n",
        "                                                                                                    user_reviews,item_reviews)\n",
        "\n",
        "    loss_val = train_step(my_model, user_batch, item_batch, rates_batch, epoch)\n",
        "    loss_total += loss_val\n",
        "    count += 1.0\n",
        "\n",
        "  t1 = time()\n",
        "  mses, maes = [], []\n",
        "  for i in range(len(user_input_test)):\n",
        "      mses, maes = eval_model(my_model, user_tests[i], item_tests[i], rating_input_test[i], mses, maes)\n",
        "  mse = np.array(mses).mean()\n",
        "  mae = np.array(maes).mean()\n",
        "  t2 = time()\n",
        "\n",
        "  print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f  mse = %.3f  mae = %.3f\"%(epoch, (t1 - t), (t2 - t1), loss_total/count, mse, mae))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch0 train time: 24.762s test time: 21.665  loss = 13.542  mse = 4.677  mae = 1.973\n",
            "epoch1 train time: 23.922s test time: 21.531  loss = 4.718  mse = 1.481  mae = 1.036\n",
            "epoch2 train time: 24.148s test time: 21.664  loss = 2.035  mse = 1.010  mae = 0.780\n",
            "epoch3 train time: 24.071s test time: 21.571  loss = 1.499  mse = 0.992  mae = 0.771\n",
            "epoch4 train time: 23.671s test time: 21.466  loss = 1.410  mse = 0.996  mae = 0.775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jSeQzLWY7dL"
      },
      "source": [
        "# Review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjAf3Z7QZw-b"
      },
      "source": [
        "# encoding: utf-8\n",
        "# import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "import math, os\n",
        "\n",
        "\n",
        "def ini_word_embed(num_words, latent_dim):\n",
        "    word_embeds = np.random.rand(num_words, latent_dim)\n",
        "    return word_embeds\n",
        "\n",
        "def word2vec_word_embed(num_words, latent_dim, path, word_id_dict):\n",
        "    word2vect_embed_mtrx = np.zeros((num_words, latent_dim))\n",
        "    with open(path, \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            row_id = word_id_dict.get(arr[0])\n",
        "            vect = arr[1].strip().split(\" \")\n",
        "            for i in range(len(vect)):\n",
        "                word2vect_embed_mtrx[row_id, i] = float(vect[i])\n",
        "            line = f.readline()\n",
        "\n",
        "    return word2vect_embed_mtrx\n",
        "\n",
        "\n",
        "def fasttext_word_embed(num_words, latent_dim, pretrained_model, word_id_dict):\n",
        "    fasttext_embed_mtrx = np.zeros((num_words, latent_dim))\n",
        "    for word in word_id_dict:\n",
        "      fasttext_embed_mtrx[word_id_dict[word]] = pretrained_model[word]\n",
        "      \n",
        "    return fasttext_embed_mtrx\n",
        "\n",
        "def get_train_instance(train):\n",
        "    user_input, item_input, rates = [], [], []\n",
        "\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        rates.append(train[u,i])\n",
        "    return user_input, item_input, rates\n",
        "\n",
        "def get_train_instance_batch_change(count, batch_size, user_input, item_input, ratings, user_reviews, item_reviews):\n",
        "    users_batch, items_batch, user_input_batch, item_input_batch, labels_batch = [], [], [], [], []\n",
        "\n",
        "    for idx in range(batch_size):\n",
        "        index = (count*batch_size + idx) % len(user_input)\n",
        "        users_batch.append(user_input[index])\n",
        "        items_batch.append(item_input[index])\n",
        "        user_input_batch.append(user_reviews.get(user_input[index]))\n",
        "        item_input_batch.append(item_reviews.get(item_input[index]))\n",
        "        labels_batch.append([ratings[index]])\n",
        "\n",
        "    return users_batch, items_batch, user_input_batch, item_input_batch, labels_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEXAnt1NnJZE",
        "outputId": "c0f612d7-9d48-46ab-b020-42db20de97de"
      },
      "source": [
        "import numpy as np\n",
        "from time import time\n",
        "import math, os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "word_latent_dim = 300\n",
        "latent_dim = 30\n",
        "max_doc_length = 300\n",
        "# windows = 3\n",
        "num_filters = 50\n",
        "window_size = 3\n",
        "v_dim = 50\n",
        "learning_rate = 0.001\n",
        "lambda_1 = 0.01\n",
        "# batch_size = 100\n",
        "drop_out = 0.8\n",
        "batch_size = 200\n",
        "epochs = 180\n",
        "\n",
        "\n",
        "# loading data\n",
        "firTime = time()\n",
        "dataset_path = \"Dataset/\"\n",
        "category = \"patio\"\n",
        "path = dataset_path + category + \"/\"\n",
        "dataSet = Dataset(max_doc_length, path,\"WordDict.out\")\n",
        "word_dict, user_reviews, item_reviews, train, valRatings, testRatings = dataSet.word_id_dict, dataSet.userReview_dict, dataSet.itemReview_dict, dataSet.trainMtrx, dataSet.valRatings, dataSet.testRatings\n",
        "secTime = time()\n",
        "\n",
        "\n",
        "num_users, num_items = train.shape\n",
        "print(\"load data: %.3fs\" % (secTime - firTime))\n",
        "print(num_users, num_items)\n",
        "\n",
        "# load word embeddings\n",
        "word_embedding_mtrx = fasttext_word_embed(len(word_dict), word_latent_dim, fasttext_model, word_dict)\n",
        "print(\"shape\", word_embedding_mtrx.shape)\n",
        "print(word_embedding_mtrx)\n",
        "\n",
        "# get train instances\n",
        "user_input, item_input, rateings = get_train_instance(train)\n",
        "\n",
        "# get test instances\n",
        "user_vals, item_vals, user_input_val, item_input_val, rating_input_val = get_test_list(200, valRatings, user_reviews, item_reviews)\n",
        "user_tests, item_tests, user_input_test, item_input_test, rating_input_test = get_test_list(200, testRatings, user_reviews, item_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wordId_dict finished\n",
            "load reviews finished\n",
            "22363 12101\n",
            "load data: 15.869s\n",
            "22363 12101\n",
            "shape (189590, 300)\n",
            "[[ 0.34721792 -0.23254557  0.08277421 ...  0.5184263  -0.15084337\n",
            "  -0.22017725]\n",
            " [-0.25729463 -0.24384813 -0.29711977 ... -0.09914862 -0.69585395\n",
            "   0.21937728]\n",
            " [-0.18768255 -0.40687621  0.11908793 ...  0.28632459 -0.0473632\n",
            "  -0.12526543]\n",
            " ...\n",
            " [ 0.0032126  -0.00386322  0.00366708 ... -0.00490303 -0.00736631\n",
            "  -0.00388915]\n",
            " [-0.00172517  0.01667362  0.00282218 ... -0.00159003 -0.00430298\n",
            "  -0.00606333]\n",
            " [-0.00323265  0.00319192  0.00483765 ...  0.00491108  0.00687607\n",
            "  -0.00377376]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAHA4YXtjorB"
      },
      "source": [
        "def cnn_model_average(filters, user_reviews_representation_expnd, item_reviews_representation_expnd, W_u, W_i, W_u_1, W_i_1, rand_matrix):\n",
        "    #convolution layer\n",
        "    convU = tf.nn.conv2d(user_reviews_representation_expnd, W_u, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "    convI = tf.nn.conv2d(item_reviews_representation_expnd, W_i, strides=[1, 1, word_latent_dim, 1], padding='SAME')\n",
        "\n",
        "    hU = tf.nn.relu(tf.squeeze(convU, 2))\n",
        "    hI = tf.nn.relu(tf.squeeze(convI, 2))\n",
        "\n",
        "    #attentive layer\n",
        "    sec_dim = int(hU.get_shape()[1])\n",
        "    tmphU = tf.reshape(hU, [-1, filters])\n",
        "    hU_mul_rand = tf.reshape(tf.matmul(tmphU, rand_matrix), [-1, sec_dim, filters])\n",
        "    f = tf.matmul(hU_mul_rand, hI, transpose_b=True)\n",
        "    f = tf.expand_dims(f, -1)\n",
        "    att1 = tf.tanh(f)\n",
        "\n",
        "    pool_user = tf.reduce_mean(att1, 2)\n",
        "    pool_item = tf.reduce_mean(att1, 1)\n",
        "\n",
        "    user_flat = tf.squeeze(pool_user, -1)\n",
        "    item_flat = tf.squeeze(pool_item, -1)\n",
        "    weight_user = tf.nn.softmax(user_flat)\n",
        "    weight_item = tf.nn.softmax(item_flat)\n",
        "\n",
        "    weight_user_exp = tf.expand_dims(weight_user, -1)\n",
        "    weight_item_exp = tf.expand_dims(weight_item, -1)\n",
        "\n",
        "    hU = tf.expand_dims(hU * weight_user_exp, -1)\n",
        "    hI = tf.expand_dims(hI * weight_item_exp, -1)\n",
        "\n",
        "    #abstracting layer\n",
        "    hU_1 = tf.nn.conv2d(hU, W_u_1, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    hI_1 = tf.nn.conv2d(hI, W_i_1, strides=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "    sec_dim = hU_1.get_shape()[1]\n",
        "\n",
        "    oU = tf.nn.avg_pool(hU_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "    oI = tf.nn.avg_pool(hI_1, ksize=[1, sec_dim, 1, 1], strides=[1, 1, 1, 1],\n",
        "        padding='VALID')\n",
        "\n",
        "    att_user = tf.squeeze(oU)\n",
        "    att_item = tf.squeeze(oI)\n",
        "    #print \"attention\", att_user.get_shape(), att_item.get_shape()\n",
        "\n",
        "    return att_user, att_item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUYUC7BXD0VE"
      },
      "source": [
        "class Review_Model():\n",
        "  def __init__(self, word_embedding_mtrx, dropout_rate):\n",
        "    self.text_embedding = tf.Variable(word_embedding_mtrx, dtype=tf.float32, name=\"review_text_embeds\")\n",
        "    self.padding_embedding = tf.Variable(np.zeros([1, word_latent_dim]), dtype=tf.float32)\n",
        "    self.text_mask = tf.constant([1.0] * self.text_embedding.get_shape()[0] + [0.0])\n",
        "    self.user_bias = tf.Variable(tf.random.normal([num_users, 1], mean=0, stddev=0.02), name=\"review_user_bias\")\n",
        "    self.item_bias = tf.Variable(tf.random.normal([num_items, 1], mean=0, stddev=0.02), name=\"review_item_bias\")\n",
        "    self.word_embeddings = tf.concat([self.text_embedding, self.padding_embedding], 0) * tf.expand_dims(self.text_mask, -1)\n",
        "\n",
        "    self.W_u = tf.Variable(\n",
        "        tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_u\")\n",
        "    self.W_i = tf.Variable(\n",
        "        tf.random.truncated_normal([window_size, word_latent_dim, 1, num_filters], stddev=0.3), name=\"review_W_i\")\n",
        "    self.W_u_1 = tf.Variable(\n",
        "        tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_u_1\")\n",
        "    self.W_i_1 = tf.Variable(\n",
        "        tf.random.truncated_normal([window_size, num_filters, 1, num_filters], stddev=0.3), name=\"review_W_i_1\")\n",
        "    # b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]))\n",
        "    self.rand_matrix = tf.Variable(tf.random.truncated_normal([num_filters, num_filters], stddev=0.3), name=\"review_rand_matrix\")\n",
        "\n",
        "    #shared mlp layer\n",
        "    self.W_mlp = tf.nn.dropout(tf.Variable(tf.random.normal([num_filters, latent_dim], mean=0, stddev=0.02), name=\"review_W_mlp\"), dropout_rate)\n",
        "    self.b_mlp = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name=\"review_b_mlp\")\n",
        "\n",
        "    #FM predictor\n",
        "    self.w_0 = tf.Variable(tf.zeros(1), name=\"review_w_0\")\n",
        "    self.w_1 = tf.Variable(tf.random.truncated_normal([1, latent_dim * 3], stddev=0.3), name=\"review_w_1\")\n",
        "    self.v = tf.Variable(tf.random.truncated_normal([latent_dim * 3, v_dim], stddev=0.3), name=\"review_v\")\n",
        "\n",
        "    self.user_bs = None\n",
        "    self.item_bs = None\n",
        "    \n",
        "  def run(self, users, items, users_inputs, items_inputs):\n",
        "    self.user_bs = tf.nn.embedding_lookup(self.user_bias, users)\n",
        "    self.item_bs = tf.nn.embedding_lookup(self.item_bias, items)\n",
        "\n",
        "    user_reviews_representation = tf.nn.embedding_lookup(self.word_embeddings, users_inputs)\n",
        "    user_reviews_representation_expnd = tf.expand_dims(user_reviews_representation, -1)\n",
        "    item_reviews_representation = tf.nn.embedding_lookup(self.word_embeddings, items_inputs)\n",
        "    item_reviews_representation_expnd = tf.expand_dims(item_reviews_representation, -1)\n",
        "\n",
        "    user_embeds, item_embeds = cnn_model_average(num_filters, user_reviews_representation_expnd, item_reviews_representation_expnd, self.W_u, self.W_i, self.W_u_1, self.W_i_1, self.rand_matrix)\n",
        "    \n",
        "    user_embeds = tf.nn.relu(tf.matmul(user_embeds, self.W_mlp) + self.b_mlp)\n",
        "    item_embeds = tf.nn.relu(tf.matmul(item_embeds, self.W_mlp) + self.b_mlp)\n",
        "\n",
        "    embeds_sum = tf.concat([tf.multiply(user_embeds, item_embeds), user_embeds, item_embeds], 1, name=\"concat_embed\")\n",
        "\n",
        "    J_1 = self.w_0 + tf.matmul(embeds_sum, self.w_1, transpose_b=True)\n",
        "\n",
        "    embeds_sum_1 = tf.expand_dims(embeds_sum, -1)\n",
        "    embeds_sum_2 = tf.expand_dims(embeds_sum, 1)\n",
        "\n",
        "    J_2 = tf.reduce_sum(\n",
        "        tf.reduce_sum(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(self.v, self.v, transpose_b=True)),\n",
        "                      2), 1, keepdims=True)\n",
        "    J_3 = tf.linalg.trace(tf.multiply(tf.matmul(embeds_sum_1, embeds_sum_2), tf.matmul(self.v, self.v, transpose_b=True)))\n",
        "    J_total = (J_1 + 0.5 * (J_2 - J_3))\n",
        "    outputs = J_total + self.user_bs + self.item_bs\n",
        "    return outputs\n",
        "\n",
        "  def trainable_variables(self):\n",
        "    return [self.user_bias, self.item_bias, self.W_u, self.W_i, self.W_u_1, self.W_i_1, self.w_0, self.w_1, self.v]\n",
        "    \n",
        "  def loss_function(self,y_pred,y_true):\n",
        "    return tf.reduce_mean(tf.math.squared_difference(y_pred, y_true)) + lambda_1 * (tf.nn.l2_loss(self.W_i_1) + tf.nn.l2_loss(self.W_u_1) + tf.nn.l2_loss(self.W_u) + tf.nn.l2_loss(self.W_i) + tf.nn.l2_loss(self.v) + tf.nn.l2_loss(self.rand_matrix) + tf.nn.l2_loss(self.user_bs) + tf.nn.l2_loss(self.item_bs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6YNUTiRgR5b"
      },
      "source": [
        "my_review_model = Review_Model(word_embedding_mtrx,drop_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XomcDshvgYuN"
      },
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nZKPo4QgcHq"
      },
      "source": [
        "def train_step( model, users, items , users_inputs, items_inputs, y_true, epoch):\n",
        "    \n",
        "  with tf.GradientTape() as tape:\n",
        "        \n",
        "    # Get the predictions\n",
        "    preds = model.run(users, items, users_inputs, items_inputs)\n",
        "        \n",
        "    # Calc the loss\n",
        "    current_loss = model.loss_function(preds,y_true)\n",
        "    \n",
        "    # Get the gradients\n",
        "    grads = tape.gradient(current_loss, model.trainable_variables())\n",
        "    \n",
        "    # Update the weights\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables()))\n",
        "\n",
        "    return current_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m17dqpCGhx9w"
      },
      "source": [
        "def eval_model(model, user_test, item_test, users_inputs_test, items_inputs_test, rate_tests, rmses, maes):\n",
        "\n",
        "    # predicts = sess.run(predict_rating, feed_dict={users: user_test, items: item_test})\n",
        "    predicts = model.run(user_test, item_test, users_inputs_test, items_inputs_test)\n",
        "    row, col = predicts.shape\n",
        "    for r in range(row):\n",
        "        rmses.append(pow((predicts[r, 0] - rate_tests[r][0]), 2))\n",
        "        maes.append(abs(predicts[r, 0] - rate_tests[r][0]))\n",
        "    return rmses, maes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y60T7a9iNWx",
        "outputId": "ec9b1c3f-6688-46a7-e7a0-e70840104213"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  t = time()\n",
        "  loss_total = 0.0\n",
        "  count = 0.0\n",
        "          \n",
        "  for i in range(int(math.ceil(len(user_input) / float(batch_size)))):\n",
        "    user_batch, item_batch, user_input_batch, item_input_batch, rates_batch = get_train_instance_batch_change(i, batch_size,user_input,\n",
        "                                                                                                    item_input, rateings,\n",
        "                                                                                                    user_reviews,item_reviews)\n",
        "\n",
        "    loss_val = train_step(my_review_model, user_batch, item_batch, user_input_batch, item_input_batch, rates_batch, epoch)\n",
        "    loss_total += loss_val\n",
        "    count += 1.0\n",
        "\n",
        "  t1 = time()\n",
        "  mses, maes = [], []\n",
        "  for i in range(len(user_input_test)):\n",
        "      mses, maes = eval_model(my_review_model, user_tests[i], item_tests[i], user_input_test[i], item_input_test[i], rating_input_test[i], mses, maes)\n",
        "  mse = np.array(mses).mean()\n",
        "  mae = np.array(maes).mean()\n",
        "  t2 = time()\n",
        "\n",
        "  print(\"epoch%d train time: %.3fs test time: %.3f  loss = %.3f  mse = %.3f  mae = %.3f\"%(epoch, (t1 - t), (t2 - t1), loss_total/count, mse, mae))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch0 train time: 98.271s test time: 30.998  loss = 10.932  mse = 1.706  mae = 0.837\n",
            "epoch1 train time: 66.730s test time: 31.522  loss = 2.761  mse = 1.605  mae = 0.832\n",
            "epoch2 train time: 66.165s test time: 30.670  loss = 2.419  mse = 1.542  mae = 0.826\n",
            "epoch3 train time: 66.915s test time: 32.188  loss = 2.241  mse = 1.500  mae = 0.818\n",
            "epoch4 train time: 67.427s test time: 30.833  loss = 2.147  mse = 1.475  mae = 0.810\n",
            "epoch5 train time: 66.616s test time: 30.950  loss = 2.094  mse = 1.464  mae = 0.803\n",
            "epoch6 train time: 66.074s test time: 30.795  loss = 2.062  mse = 1.461  mae = 0.796\n",
            "epoch7 train time: 65.754s test time: 30.589  loss = 2.039  mse = 1.463  mae = 0.793\n",
            "epoch8 train time: 65.510s test time: 30.657  loss = 2.020  mse = 1.463  mae = 0.792\n",
            "epoch9 train time: 66.419s test time: 31.268  loss = 2.004  mse = 1.466  mae = 0.792\n",
            "epoch10 train time: 65.930s test time: 31.180  loss = 1.991  mse = 1.433  mae = 0.789\n",
            "epoch11 train time: 66.238s test time: 30.899  loss = 1.979  mse = 1.397  mae = 0.787\n",
            "epoch12 train time: 66.735s test time: 31.085  loss = 1.968  mse = 1.279  mae = 0.799\n",
            "epoch13 train time: 65.670s test time: 30.701  loss = 1.959  mse = 1.282  mae = 0.796\n",
            "epoch14 train time: 66.175s test time: 30.490  loss = 1.954  mse = 1.238  mae = 0.812\n",
            "epoch15 train time: 65.729s test time: 30.790  loss = 1.951  mse = 1.244  mae = 0.807\n",
            "epoch16 train time: 66.737s test time: 31.177  loss = 1.948  mse = 1.251  mae = 0.802\n",
            "epoch17 train time: 66.049s test time: 30.404  loss = 1.946  mse = 1.266  mae = 0.797\n",
            "epoch18 train time: 65.357s test time: 30.594  loss = 1.944  mse = 1.296  mae = 0.789\n",
            "epoch19 train time: 65.572s test time: 30.868  loss = 1.943  mse = 1.292  mae = 0.789\n",
            "epoch20 train time: 65.554s test time: 30.654  loss = 1.942  mse = 1.266  mae = 0.795\n",
            "epoch21 train time: 65.975s test time: 30.874  loss = 1.940  mse = 1.244  mae = 0.803\n",
            "epoch22 train time: 65.671s test time: 30.612  loss = 1.939  mse = 1.332  mae = 0.784\n",
            "epoch23 train time: 65.494s test time: 30.550  loss = 1.939  mse = 1.292  mae = 0.788\n",
            "epoch24 train time: 66.235s test time: 30.941  loss = 1.938  mse = 1.280  mae = 0.790\n",
            "epoch25 train time: 65.403s test time: 30.225  loss = 1.937  mse = 1.236  mae = 0.806\n",
            "epoch26 train time: 67.622s test time: 32.092  loss = 1.936  mse = 1.251  mae = 0.798\n",
            "epoch27 train time: 68.412s test time: 31.711  loss = 1.936  mse = 1.291  mae = 0.788\n",
            "epoch28 train time: 68.292s test time: 31.033  loss = 1.936  mse = 1.276  mae = 0.791\n",
            "epoch29 train time: 68.283s test time: 31.174  loss = 1.935  mse = 1.327  mae = 0.784\n",
            "epoch30 train time: 69.942s test time: 31.581  loss = 1.935  mse = 1.341  mae = 0.784\n",
            "epoch31 train time: 68.758s test time: 31.297  loss = 1.935  mse = 1.330  mae = 0.784\n",
            "epoch32 train time: 68.430s test time: 31.558  loss = 1.934  mse = 1.306  mae = 0.785\n",
            "epoch33 train time: 68.578s test time: 31.584  loss = 1.934  mse = 1.324  mae = 0.784\n",
            "epoch34 train time: 68.083s test time: 31.151  loss = 1.934  mse = 1.230  mae = 0.807\n",
            "epoch35 train time: 67.326s test time: 31.087  loss = 1.934  mse = 1.264  mae = 0.793\n",
            "epoch36 train time: 67.318s test time: 31.087  loss = 1.933  mse = 1.322  mae = 0.784\n",
            "epoch37 train time: 67.115s test time: 31.326  loss = 1.933  mse = 1.276  mae = 0.790\n",
            "epoch38 train time: 68.354s test time: 31.325  loss = 1.933  mse = 1.307  mae = 0.785\n",
            "epoch39 train time: 68.517s test time: 31.765  loss = 1.933  mse = 1.344  mae = 0.784\n",
            "epoch40 train time: 68.402s test time: 31.214  loss = 1.933  mse = 1.289  mae = 0.787\n",
            "epoch41 train time: 68.365s test time: 31.540  loss = 1.933  mse = 1.246  mae = 0.799\n",
            "epoch42 train time: 68.601s test time: 31.838  loss = 1.933  mse = 1.320  mae = 0.784\n",
            "epoch43 train time: 68.809s test time: 31.670  loss = 1.933  mse = 1.229  mae = 0.808\n",
            "epoch44 train time: 68.907s test time: 31.429  loss = 1.933  mse = 1.232  mae = 0.806\n",
            "epoch45 train time: 68.814s test time: 31.726  loss = 1.933  mse = 1.229  mae = 0.808\n",
            "epoch46 train time: 68.554s test time: 31.588  loss = 1.932  mse = 1.282  mae = 0.788\n",
            "epoch47 train time: 69.229s test time: 31.773  loss = 1.933  mse = 1.266  mae = 0.792\n",
            "epoch48 train time: 69.142s test time: 32.152  loss = 1.933  mse = 1.231  mae = 0.806\n",
            "epoch49 train time: 68.732s test time: 31.744  loss = 1.933  mse = 1.261  mae = 0.793\n",
            "epoch50 train time: 69.400s test time: 31.503  loss = 1.932  mse = 1.284  mae = 0.788\n",
            "epoch51 train time: 67.999s test time: 31.112  loss = 1.933  mse = 1.249  mae = 0.798\n",
            "epoch52 train time: 67.604s test time: 30.985  loss = 1.933  mse = 1.226  mae = 0.809\n",
            "epoch53 train time: 68.058s test time: 31.183  loss = 1.932  mse = 1.229  mae = 0.808\n",
            "epoch54 train time: 67.900s test time: 31.224  loss = 1.932  mse = 1.283  mae = 0.788\n",
            "epoch55 train time: 67.884s test time: 30.575  loss = 1.933  mse = 1.228  mae = 0.809\n",
            "epoch56 train time: 67.747s test time: 30.950  loss = 1.932  mse = 1.285  mae = 0.788\n",
            "epoch57 train time: 68.050s test time: 31.370  loss = 1.933  mse = 1.238  mae = 0.802\n",
            "epoch58 train time: 67.746s test time: 30.987  loss = 1.932  mse = 1.230  mae = 0.807\n",
            "epoch59 train time: 67.872s test time: 30.908  loss = 1.932  mse = 1.283  mae = 0.788\n",
            "epoch60 train time: 67.820s test time: 30.578  loss = 1.932  mse = 1.228  mae = 0.808\n",
            "epoch61 train time: 67.698s test time: 31.062  loss = 1.932  mse = 1.227  mae = 0.809\n",
            "epoch62 train time: 66.975s test time: 30.930  loss = 1.932  mse = 1.228  mae = 0.808\n",
            "epoch63 train time: 67.199s test time: 30.523  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch64 train time: 67.913s test time: 30.999  loss = 1.932  mse = 1.243  mae = 0.800\n",
            "epoch65 train time: 68.043s test time: 31.192  loss = 1.932  mse = 1.233  mae = 0.805\n",
            "epoch66 train time: 67.474s test time: 30.691  loss = 1.932  mse = 1.233  mae = 0.805\n",
            "epoch67 train time: 68.117s test time: 31.201  loss = 1.932  mse = 1.241  mae = 0.801\n",
            "epoch68 train time: 67.486s test time: 31.355  loss = 1.932  mse = 1.224  mae = 0.810\n",
            "epoch69 train time: 67.841s test time: 30.359  loss = 1.932  mse = 1.223  mae = 0.812\n",
            "epoch70 train time: 67.175s test time: 30.808  loss = 1.932  mse = 1.224  mae = 0.810\n",
            "epoch71 train time: 67.571s test time: 31.159  loss = 1.932  mse = 1.238  mae = 0.802\n",
            "epoch72 train time: 67.919s test time: 31.048  loss = 1.932  mse = 1.225  mae = 0.810\n",
            "epoch73 train time: 67.755s test time: 31.137  loss = 1.932  mse = 1.255  mae = 0.796\n",
            "epoch74 train time: 67.896s test time: 30.932  loss = 1.932  mse = 1.226  mae = 0.809\n",
            "epoch75 train time: 67.977s test time: 31.085  loss = 1.932  mse = 1.223  mae = 0.812\n",
            "epoch76 train time: 67.720s test time: 30.772  loss = 1.932  mse = 1.224  mae = 0.810\n",
            "epoch77 train time: 67.771s test time: 30.702  loss = 1.932  mse = 1.233  mae = 0.805\n",
            "epoch78 train time: 66.891s test time: 30.225  loss = 1.932  mse = 1.224  mae = 0.810\n",
            "epoch79 train time: 67.282s test time: 30.549  loss = 1.932  mse = 1.225  mae = 0.811\n",
            "epoch80 train time: 69.724s test time: 32.306  loss = 1.932  mse = 1.229  mae = 0.807\n",
            "epoch81 train time: 69.085s test time: 31.326  loss = 1.932  mse = 1.223  mae = 0.812\n",
            "epoch82 train time: 68.872s test time: 31.779  loss = 1.932  mse = 1.224  mae = 0.810\n",
            "epoch83 train time: 68.896s test time: 31.658  loss = 1.932  mse = 1.223  mae = 0.812\n",
            "epoch84 train time: 68.047s test time: 30.801  loss = 1.932  mse = 1.226  mae = 0.809\n",
            "epoch85 train time: 67.652s test time: 31.067  loss = 1.932  mse = 1.224  mae = 0.812\n",
            "epoch86 train time: 66.447s test time: 30.647  loss = 1.932  mse = 1.245  mae = 0.799\n",
            "epoch87 train time: 65.926s test time: 30.440  loss = 1.932  mse = 1.226  mae = 0.810\n",
            "epoch88 train time: 65.307s test time: 30.587  loss = 1.932  mse = 1.225  mae = 0.810\n",
            "epoch89 train time: 65.435s test time: 30.536  loss = 1.932  mse = 1.223  mae = 0.812\n",
            "epoch90 train time: 65.255s test time: 30.264  loss = 1.932  mse = 1.252  mae = 0.796\n",
            "epoch91 train time: 65.245s test time: 30.260  loss = 1.932  mse = 1.226  mae = 0.810\n",
            "epoch92 train time: 65.218s test time: 30.690  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch93 train time: 65.677s test time: 30.371  loss = 1.932  mse = 1.225  mae = 0.811\n",
            "epoch94 train time: 65.241s test time: 30.346  loss = 1.932  mse = 1.242  mae = 0.800\n",
            "epoch95 train time: 65.385s test time: 30.395  loss = 1.932  mse = 1.225  mae = 0.810\n",
            "epoch96 train time: 65.533s test time: 30.278  loss = 1.932  mse = 1.223  mae = 0.811\n",
            "epoch97 train time: 65.449s test time: 30.628  loss = 1.932  mse = 1.246  mae = 0.799\n",
            "epoch98 train time: 65.624s test time: 30.397  loss = 1.932  mse = 1.225  mae = 0.810\n",
            "epoch99 train time: 67.443s test time: 30.763  loss = 1.932  mse = 1.223  mae = 0.812\n",
            "epoch100 train time: 66.471s test time: 30.955  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch101 train time: 67.550s test time: 30.564  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch102 train time: 67.247s test time: 31.246  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch103 train time: 67.297s test time: 31.313  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch104 train time: 66.409s test time: 30.695  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch105 train time: 66.887s test time: 30.926  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch106 train time: 66.863s test time: 30.913  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch107 train time: 66.744s test time: 30.448  loss = 1.932  mse = 1.225  mae = 0.810\n",
            "epoch108 train time: 66.026s test time: 30.879  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch109 train time: 65.260s test time: 30.715  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch110 train time: 65.860s test time: 30.222  loss = 1.932  mse = 1.230  mae = 0.806\n",
            "epoch111 train time: 65.179s test time: 30.496  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch112 train time: 65.275s test time: 30.811  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch113 train time: 66.937s test time: 30.488  loss = 1.932  mse = 1.224  mae = 0.811\n",
            "epoch114 train time: 66.511s test time: 30.760  loss = 1.932  mse = 1.224  mae = 0.811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN_nwR42kydU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}